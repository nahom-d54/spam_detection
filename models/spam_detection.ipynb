{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dd14b88"
      },
      "source": [
        "# Task\n",
        "Build, train, and evaluate a spam classification model using the `enron_spam_data.csv` dataset, including data preprocessing, TF-IDF feature extraction, and evaluation with metrics like accuracy, precision, recall, F1-score, and a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77b30c23",
        "outputId": "2da318d7-3bd9-4b91-c759-f6f2fe6c9bf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows of the DataFrame:\n",
            "   Message ID                       Subject  \\\n",
            "0           0  christmas tree farm pictures   \n",
            "1           1      vastar resources , inc .   \n",
            "2           2  calpine daily gas nomination   \n",
            "3           3                    re : issue   \n",
            "4           4     meter 7268 nov allocation   \n",
            "\n",
            "                                             Message Spam/Ham        Date  \n",
            "0                                                NaN      ham  1999-12-10  \n",
            "1  gary , production from the high island larger ...      ham  1999-12-13  \n",
            "2             - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n",
            "3  fyi - see note below - already done .\\nstella\\...      ham  1999-12-14  \n",
            "4  fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham  1999-12-14  \n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 33716 entries, 0 to 33715\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Message ID  33716 non-null  int64 \n",
            " 1   Subject     33427 non-null  object\n",
            " 2   Message     33345 non-null  object\n",
            " 3   Spam/Ham    33716 non-null  object\n",
            " 4   Date        33716 non-null  object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 1.3+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# SOURCE: https://www.kaggle.com/datasets/marcelwiechmann/enron-spam-data?select=enron_spam_data.csv\n",
        "\n",
        "# Construct the file path using the gdrive_extracted_data_path variable\n",
        "file_path = \"enron_spam_data.csv\"\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Print a concise summary of the DataFrame\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72a7485b"
      },
      "source": [
        "## Data Preprocessing and Exploration\n",
        "\n",
        "Perform initial exploration of the dataset, clean and preprocess the text data for model training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4114529b"
      },
      "source": [
        "First, We need to check for missing values in the DataFrame to understand the data completeness. Then, we will drop the 'Message ID' column as it's not relevant for classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0afce718",
        "outputId": "6e0772d0-1126-498a-e918-57625b04d401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing values before cleaning:\n",
            "Message ID      0\n",
            "Subject       289\n",
            "Message       371\n",
            "Spam/Ham        0\n",
            "Date            0\n",
            "dtype: int64\n",
            "\n",
            "DataFrame after dropping 'Message ID' column:\n",
            "                        Subject  \\\n",
            "0  christmas tree farm pictures   \n",
            "1      vastar resources , inc .   \n",
            "2  calpine daily gas nomination   \n",
            "3                    re : issue   \n",
            "4     meter 7268 nov allocation   \n",
            "\n",
            "                                             Message Spam/Ham        Date  \n",
            "0                                                NaN      ham  1999-12-10  \n",
            "1  gary , production from the high island larger ...      ham  1999-12-13  \n",
            "2             - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n",
            "3  fyi - see note below - already done .\\nstella\\...      ham  1999-12-14  \n",
            "4  fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham  1999-12-14  \n"
          ]
        }
      ],
      "source": [
        "print(\"Missing values before cleaning:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Drop the 'Message ID' column\n",
        "df = df.drop('Message ID', axis=1)\n",
        "\n",
        "print(\"\\nDataFrame after dropping 'Message ID' column:\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c04dc6a"
      },
      "source": [
        "we will fill missing values in 'Subject' and 'Message' columns with empty strings, create a combined 'text' column from 'Subject' and 'Message', and then convert the 'Spam/Ham' column into numerical labels ('label')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7a65939",
        "outputId": "23a37f80-ba27-435d-fc8d-d723ddfb9f19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame after filling missing values, creating 'text' and 'label' columns:\n",
            "                                                text  label\n",
            "0                      christmas tree farm pictures       0\n",
            "1  vastar resources , inc . gary , production fro...      0\n",
            "2  calpine daily gas nomination - calpine daily g...      0\n",
            "3  re : issue fyi - see note below - already done...      0\n",
            "4  meter 7268 nov allocation fyi .\\n- - - - - - -...      0\n"
          ]
        }
      ],
      "source": [
        "df['Subject'] = df['Subject'].fillna('')\n",
        "df['Message'] = df['Message'].fillna('')\n",
        "\n",
        "# Create a new column 'text' by concatenating 'Subject' and 'Message'\n",
        "df['text'] = df['Subject'] + ' ' + df['Message']\n",
        "\n",
        "# Convert 'Spam/Ham' to numerical labels\n",
        "df['label'] = df['Spam/Ham'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "print(\"DataFrame after filling missing values, creating 'text' and 'label' columns:\")\n",
        "print(df[['text', 'label']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "febd3544"
      },
      "source": [
        "\n",
        "To preprocess the text data we need to import necessary NLTK modules, download the required 'stopwords', 'wordnet', and 'omw-1.4' corpora, and then define a text cleaning function that performs lowercasing, punctuation removal, digit removal, tokenization, stop word removal, and lemmatization. Finally, we will apply this function to the 'text' column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bde7a61",
        "outputId": "f9dfe9e1-173d-4230-b95a-d2da2a89beba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame after applying text preprocessing:\n",
            "                                                text  label\n",
            "0                        christmas tree farm picture      0\n",
            "1  vastar resource inc gary production high islan...      0\n",
            "2  calpine daily gas nomination calpine daily gas...      0\n",
            "3  issue fyi see note already done stella forward...      0\n",
            "4  meter nov allocation fyi forwarded lauri allen...      0\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added this download\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text) # [Corrected, digit, removal ,to, handle ,multiple ,digits]\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words and lemmatize\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "            cleaned_tokens.append(lemmatizer.lemmatize(token))\n",
        "    # Join back into string\n",
        "    return ' '.join(cleaned_tokens)\n",
        "\n",
        "# Apply the preprocessing function to the 'text' column\n",
        "df['text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"DataFrame after applying text preprocessing:\")\n",
        "print(df[['text', 'label']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "893faa05"
      },
      "source": [
        "# Task\n",
        "Convert the preprocessed text data into numerical features using TF-IDF Vectorizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b2556ed"
      },
      "source": [
        "## Feature Extraction (TF-IDF)\n",
        "\n",
        "Convert the preprocessed text data into numerical features using a TF-IDF Vectorizer, which is suitable for text classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e95322c",
        "outputId": "0378eede-774a-4943-c8fb-e3adc7d84ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of TF-IDF features: (33716, 5000)\n",
            "Sample TF-IDF features (first 5 rows, first 10 columns):\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.03592908 0.33767781 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.03165009\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.07265102\n",
            "  0.         0.03947017 0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Limiting to 5000 features for demonstration\n",
        "\n",
        "# Apply TF-IDF to the preprocessed text data\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(df['text'])\n",
        "\n",
        "print(\"Shape of TF-IDF features:\", tfidf_features.shape)\n",
        "print(\"Sample TF-IDF features (first 5 rows, first 10 columns):\")\n",
        "print(tfidf_features[:5, :10].toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13b24452"
      },
      "source": [
        "## Split Data into Training and Testing Sets\n",
        "\n",
        "Divide the dataset into training and testing subsets to train the model on one part and evaluate its performance on unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b5398f5",
        "outputId": "01a4f089-93c0-40b4-e9f4-5907ceb9922a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train: (26972, 5000)\n",
            "Shape of X_test: (6744, 5000)\n",
            "Shape of y_train: (26972,)\n",
            "Shape of y_test: (6744,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = tfidf_features\n",
        "y = df['label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Print the shapes of the resulting datasets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d2fa526"
      },
      "source": [
        "## Train a Spam Classifier Model\n",
        "\n",
        "Train a classification model, such as Multinomial Naive Bayes on the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "009320ab",
        "outputId": "16cfe5ae-fdf6-4b79-f1cb-3b5938d0ab19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multinomial Naive Bayes model trained successfully.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Instantiate a Multinomial Naive Bayes classifier\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Multinomial Naive Bayes model trained successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d37a4e22"
      },
      "source": [
        "## Evaluate Model Performance\n",
        "\n",
        "Evaluate the trained model's performance using appropriate metrics like accuracy, precision, recall, and F1-score, and display a confusion matrix for detailed insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41812aa5",
        "outputId": "9b0f48ea-aec7-4ad4-e24b-e15fe502a8c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9862\n",
            "Precision: 0.9807\n",
            "Recall: 0.9924\n",
            "F1-score: 0.9865\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3242   67]\n",
            " [  26 3409]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-score: {f1:.4f}\")\n",
        "\n",
        "# Generate and print the confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "516fb10e"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "summary of the spam classification model's performance and any potential next steps or improvements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f23b30ef"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The spam classification model demonstrates strong performance, with an accuracy of 0.9841, precision of 0.9809, recall of 0.9881, and an F1-score of 0.9845.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   Text data was successfully converted into numerical features using a TF-IDF Vectorizer, resulting in a feature matrix of shape (33716, 5000).\n",
        "*   The dataset was split into training and testing sets with an 80/20 ratio:\n",
        "    *   Training set: 26972 samples for both features and labels.\n",
        "    *   Testing set: 6744 samples for both features and labels.\n",
        "*   A Multinomial Naive Bayes classifier was trained, achieving high performance on the test set.\n",
        "*   The model's performance metrics are:\n",
        "    *   **Accuracy**: 0.9841\n",
        "    *   **Precision**: 0.9809\n",
        "    *   **Recall**: 0.9881\n",
        "    *   **F1-score**: 0.9845\n",
        "*   The confusion matrix indicates:\n",
        "    *   3243 True Negatives (correctly identified non-spam)\n",
        "    *   3394 True Positives (correctly identified spam)\n",
        "    *   66 False Positives (non-spam incorrectly classified as spam)\n",
        "    *   41 False Negatives (spam incorrectly classified as non-spam)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c292496",
        "outputId": "7a602b16-0d09-4264-be2c-fcb0a9f587a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained model saved to: /content/gdrive/MyDrive/enron_spam_data_extracted/spam_classifier_model.joblib\n",
            "TF-IDF vectorizer saved to: /content/gdrive/MyDrive/enron_spam_data_extracted/tfidf_vectorizer.joblib\n",
            "Both the model and vectorizer have been saved successfully to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Define the file paths for saving\n",
        "model_path = os.path.join(\"\", 'spam_classifier_model.joblib')\n",
        "vectorizer_path = os.path.join(\"\",'tfidf_vectorizer.joblib')\n",
        "\n",
        "# Save the trained model\n",
        "joblib.dump(model, model_path)\n",
        "print(f\"Trained model saved to: {model_path}\")\n",
        "\n",
        "# Save the fitted TF-IDF vectorizer\n",
        "joblib.dump(tfidf_vectorizer, vectorizer_path)\n",
        "print(f\"TF-IDF vectorizer saved to: {vectorizer_path}\")\n",
        "\n",
        "print(\"Both the model and vectorizer have been saved successfully to Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf17db23",
        "outputId": "a8d0b3a9-1451-4213-9575-1c07d4a2943d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression model trained successfully.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Instantiate a Logistic Regression model\n",
        "# Using max_iter to ensure convergence, especially with large datasets or default solver\n",
        "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Logistic Regression model trained successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab3eb4ca",
        "outputId": "20548363-e8c0-4962-de1a-80cd397e80fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression model trained successfully.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Re-define features (X) and target (y) as they were not found\n",
        "X = tfidf_features\n",
        "y = df['label']\n",
        "\n",
        "# Re-split the data into training and testing sets to ensure X_train and y_train are defined\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Instantiate a Logistic Regression model\n",
        "# Using max_iter to ensure convergence, especially with large datasets or default solver\n",
        "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "logistic_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Logistic Regression model trained successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18b7bb4d"
      },
      "source": [
        "# Task\n",
        "Evaluate the Logistic Regression model's performance using accuracy, precision, recall, F1-score, and a confusion matrix on the test data (`X_test`, `y_test`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f198b97c"
      },
      "source": [
        "## Evaluate Logistic Regression Model\n",
        "\n",
        "Evaluate the Logistic Regression model's performance using accuracy, precision, recall, F1-score, and a confusion matrix on the test data (`X_test`, `y_test`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3dcdf00",
        "outputId": "44f37949-5332-47f9-e424-a73385a148e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression Model Performance:\n",
            "Accuracy: 0.9889\n",
            "Precision: 0.9833\n",
            "Recall: 0.9951\n",
            "F1-score: 0.9891\n",
            "\n",
            "Confusion Matrix (Logistic Regression):\n",
            "[[3251   58]\n",
            " [  17 3418]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set using the logistic regression model\n",
        "y_pred_logistic = logistic_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics for the Logistic Regression model\n",
        "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
        "precision_logistic = precision_score(y_test, y_pred_logistic)\n",
        "recall_logistic = recall_score(y_test, y_pred_logistic)\n",
        "f1_logistic = f1_score(y_test, y_pred_logistic)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Logistic Regression Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_logistic:.4f}\")\n",
        "print(f\"Precision: {precision_logistic:.4f}\")\n",
        "print(f\"Recall: {recall_logistic:.4f}\")\n",
        "print(f\"F1-score: {f1_logistic:.4f}\")\n",
        "\n",
        "# Generate and print the confusion matrix\n",
        "conf_matrix_logistic = confusion_matrix(y_test, y_pred_logistic)\n",
        "print(\"\\nConfusion Matrix (Logistic Regression):\")\n",
        "print(conf_matrix_logistic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b748668a"
      },
      "source": [
        "## Train Random Forest Classifier\n",
        "\n",
        "Train a Random Forest Classifier model on the preprocessed training data (`X_train`, `y_train`) for spam classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7846e380",
        "outputId": "4f62f90a-7076-4531-c18e-07521c749e5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Classifier model trained successfully.\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instantiate a RandomForestClassifier model\n",
        "# Set n_estimators to 100 and random_state for reproducibility\n",
        "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Random Forest Classifier model trained successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "047ecdd3"
      },
      "source": [
        "## Evaluate Random Forest Classifier\n",
        "\n",
        "Evaluate the Random Forest Classifier model's performance using accuracy, precision, recall, F1-score, and a confusion matrix on the test data (`X_test`, `y_test`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b8e5a6f",
        "outputId": "60e45681-e41b-49df-cd5a-3d9c48277f54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Classifier Model Performance:\n",
            "Accuracy: 0.9867\n",
            "Precision: 0.9802\n",
            "Recall: 0.9939\n",
            "F1-score: 0.9870\n",
            "\n",
            "Confusion Matrix (Random Forest Classifier):\n",
            "[[3240   69]\n",
            " [  21 3414]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Make predictions on the test set using the random forest model\n",
        "y_pred_rf = random_forest_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics for the Random Forest Classifier model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "precision_rf = precision_score(y_test, y_pred_rf)\n",
        "recall_rf = recall_score(y_test, y_pred_rf)\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Random Forest Classifier Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_rf:.4f}\")\n",
        "print(f\"Precision: {precision_rf:.4f}\")\n",
        "print(f\"Recall: {recall_rf:.4f}\")\n",
        "print(f\"F1-score: {f1_rf:.4f}\")\n",
        "\n",
        "# Generate and print the confusion matrix\n",
        "conf_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "print(\"\\nConfusion Matrix (Random Forest Classifier):\")\n",
        "print(conf_matrix_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6c6542f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Logistic Regression Model Performance**:\n",
        "    *   Achieved high performance metrics: Accuracy of 0.9889, Precision of 0.9833, Recall of 0.9951, and an F1-score of 0.9891.\n",
        "    *   The confusion matrix showed 3251 True Negatives, 58 False Positives, 17 False Negatives, and 3418 True Positives, indicating a very low rate of misclassifying spam as non-spam (false negatives).\n",
        "*   **Random Forest Classifier Model Performance**:\n",
        "    *   Also demonstrated strong performance: Accuracy of 0.9867, Precision of 0.9802, Recall of 0.9939, and an F1-score of 0.9870.\n",
        "    *   The confusion matrix reported 3240 True Negatives, 69 False Positives, 21 False Negatives, and 3414 True Positives, similarly showing a low number of false negatives.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c24f592a",
        "outputId": "1081a06e-25b8-4490-8be5-722743bbb31a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression model saved to: /content/gdrive/MyDrive/enron_spam_data_extracted/logistic_regression_model.joblib\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Define the file path for saving the logistic regression model\n",
        "logistic_model_path = os.path.join(\"\", 'logistic_regression_model.joblib')\n",
        "\n",
        "# Save the trained logistic regression model\n",
        "joblib.dump(logistic_model, logistic_model_path)\n",
        "print(f\"Logistic Regression model saved to: {logistic_model_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
