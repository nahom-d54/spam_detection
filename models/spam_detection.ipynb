{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovyTAoqUphle",
        "outputId": "6c308aa2-2729-47d8-8241-8d47d9fcfd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/bayes2003/emails-for-spam-or-ham-classification-trec-2007?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 483M/483M [00:07<00:00, 69.3MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"bayes2003/emails-for-spam-or-ham-classification-trec-2007\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Dataset Acquisition from Kaggle\n",
        "\n",
        "**Why This Approach:**\n",
        "Using Kaggle datasets provides access to pre-curated, well-labeled spam/ham email data. KagleHub automates authentication and version management, ensuring reproducibility and easy dataset updates.\n",
        "\n",
        "**Technical Details:**\n",
        "- **KagleHub API**: Simplifies dataset downloading without manual authentication\n",
        "- **TREC 2007 Dataset**: Contains emails labeled as spam or ham from Text Retrieval Conference\n",
        "- **Automated Management**: Latest version is downloaded automatically, supporting versioning\n",
        "\n",
        "**Use Cases:**\n",
        "- Provides diverse email examples for robust model training\n",
        "- Complements the Enron dataset for better generalization\n",
        "- Industry-standard benchmark dataset for spam classification\n",
        "\n",
        "**Data Characteristics:**\n",
        "- Multiple email sources and spam types\n",
        "- Realistic distribution of legitimate and spam messages\n",
        "- Pre-processed labels (spam/ham)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "257735d8",
        "outputId": "d3f1bfd6-00ef-47c2-c09b-44748ac22bed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Google Drive Integration for Data Persistence\n",
        "\n",
        "**Why This Approach:**\n",
        "Google Colab's ephemeral file system requires persistent storage. Mounting Google Drive enables:\n",
        "- Saving trained models for later use\n",
        "- Storing datasets across sessions\n",
        "- Collaboration through shared drives\n",
        "\n",
        "**Technical Details:**\n",
        "- **OAuth 2.0 Authentication**: Secure, user-authorized access to Google Drive\n",
        "- **FUSE Filesystem**: Creates a bridge between Colab VM and Google Drive API\n",
        "- **Mount Point**: `/content/gdrive` maps Google Drive into Colab's file system\n",
        "- **Persistent Storage**: Files remain available for 24+ hours after Colab session ends\n",
        "\n",
        "**Advantages:**\n",
        "- No manual file uploads/downloads needed\n",
        "- Models can be accessed from production environment\n",
        "- Collaborative development support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6pcKxzWxiVS3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "gdrive_extracted_data_path = '/content/gdrive/MyDrive/trec_spam_data_extracted'\n",
        "data_1 = os.path.join(gdrive_extracted_data_path, 'email_text.csv')\n",
        "data_2 = os.path.join(gdrive_extracted_data_path, 'enron_spam_data.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Data Path Configuration\n",
        "\n",
        "**Why This Approach:**\n",
        "Defining paths at the start enables:\n",
        "- Centralized path management\n",
        "- Easy migration between environments (local, Colab, production)\n",
        "- Consistent file access across the notebook\n",
        "\n",
        "**Technical Details:**\n",
        "- **Path Variables**: Store full paths as variables for reusability\n",
        "- **Google Drive Structure**: Access data stored in organized drive folders\n",
        "- **Dual Dataset Strategy**: Support both TREC (kagglehub) and Enron datasets\n",
        "- **OS-Independent**: `os.path.join()` handles path separators across platforms\n",
        "\n",
        "**Data Organization:**\n",
        "- `email_text.csv`: TREC 2007 dataset (Kaggle)\n",
        "- `enron_spam_data.csv`: Enron corpus dataset\n",
        "- Both will be combined for comprehensive training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yL0KfVYitId",
        "outputId": "e97a4def-c5f5-4f97-d63a-e718e8aa490b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   label                                               text\n",
            "0      1  do you feel the pressure to perform and not ri...\n",
            "1      0  hi i've just updated from the gulus and i chec...\n",
            "2      1  mega authenticv i a g r a discount pricec i a ...\n",
            "3      1  hey billy it was really fun going out the othe...\n",
            "4      1  system of the home it will have the capabiliti...\n",
            "   Message ID                       Subject  \\\n",
            "0           0  christmas tree farm pictures   \n",
            "1           1      vastar resources , inc .   \n",
            "2           2  calpine daily gas nomination   \n",
            "3           3                    re : issue   \n",
            "4           4     meter 7268 nov allocation   \n",
            "\n",
            "                                             Message Spam/Ham        Date  \n",
            "0                                                NaN      ham  1999-12-10  \n",
            "1  gary , production from the high island larger ...      ham  1999-12-13  \n",
            "2             - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n",
            "3  fyi - see note below - already done .\\nstella\\...      ham  1999-12-14  \n",
            "4  fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham  1999-12-14  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df_1 = pd.read_csv(data_1)\n",
        "df_2 = pd.read_csv(data_2)\n",
        "\n",
        "print(df_1.head())\n",
        "print(df_2.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Multi-Source Data Loading and Exploration\n",
        "\n",
        "**Why This Approach:**\n",
        "Loading both datasets allows cross-validation of model performance and creates a more diverse training corpus. Exploratory output reveals data structure and potential issues.\n",
        "\n",
        "**Technical Details:**\n",
        "- **Pandas DataFrame**: Efficient columnar data structure for analysis\n",
        "- **Dual Loading**: Two separate CSVs merged later for flexibility\n",
        "- **Head Inspection**: `.head()` shows first 5 rows for quick validation\n",
        "- **Column Discovery**: Reveals data schema before processing\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "Dataset structure forms observation matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}$:\n",
        "- $n$ = number of emails (samples)\n",
        "- $m$ = number of features (columns like text, labels)\n",
        "\n",
        "**Expected Output:**\n",
        "- Column names reveal available features\n",
        "- Data types indicate encoding (string vs numeric)\n",
        "- Missing values visible in first rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-EqM4NzejDsz"
      },
      "outputs": [],
      "source": [
        "df_2 = df_2.drop(columns=[\"Message ID\", \"Date\", \"Subject\"])\n",
        "df_2[\"Spam/Ham\"] = df_2[\"Spam/Ham\"].map({\"ham\": 0, \"spam\": 1})\n",
        "\n",
        "# 3. Combine Subject and Message into a single text column\n",
        "df_2[\"text\"] = (\n",
        "    df_2[\"Message\"].fillna(\"\")\n",
        ")\n",
        "\n",
        "# 4. Remove newline characters\n",
        "df_2[\"text\"] = df_2[\"text\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Data Cleaning and Feature Engineering\n",
        "\n",
        "**Why This Approach:**\n",
        "Removes irrelevant columns, encodes labels, normalizes whitespace, and creates unified text representation for model consumption.\n",
        "\n",
        "**Technical Details:**\n",
        "\n",
        "1. **Column Dropping**: Remove non-predictive features\n",
        "   - Message ID: Unique identifier with no pattern\n",
        "   - Date: Temporal info not used by static model\n",
        "   - Subject: Merged into text column\n",
        "\n",
        "2. **Label Encoding**: Binary mapping for classification\n",
        "   - Mathematical form: $y \\in \\{0, 1\\}$ (ham=0, spam=1)\n",
        "   - Enables loss computation: $L = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$\n",
        "\n",
        "3. **Text Normalization**: Whitespace standardization\n",
        "   - Regex `\\s+` matches multiple whitespace characters\n",
        "   - Prevents tokenization errors from excessive spacing\n",
        "\n",
        "**Data Quality Improvements:**\n",
        "- Reduces feature dimensionality\n",
        "- Standardizes format for consistent processing\n",
        "- Removes noise from formatting variations\n",
        "\n",
        "**Information Content:**\n",
        "- Dropped features: Low mutual information with spam label\n",
        "- Text feature: Maximum mutual information, essential for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ZRyqCL3YnqSQ"
      },
      "outputs": [],
      "source": [
        "df_2 = df_2.rename(columns={\"Spam/Ham\": \"label\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Schema Standardization\n",
        "\n",
        "**Why This Approach:**\n",
        "Standardizes column naming across datasets for consistent concatenation. The label column should have uniform name for both datasets before merging.\n",
        "\n",
        "**Technical Details:**\n",
        "- **Column Renaming**: Ensures both df_1 and df_2 have same label column name\n",
        "- **Pre-Merge Preparation**: Critical before using `pd.concat()`\n",
        "- **API Consistency**: Makes downstream processing uniform\n",
        "\n",
        "**Impact:**\n",
        "Enables `pd.concat()` to properly merge label columns from different sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "AB3wJyXcnuCD"
      },
      "outputs": [],
      "source": [
        "# delete the null and combine with df_1 and df_2 to one df\n",
        "\n",
        "df_2 = df_2.dropna(subset=['text'])\n",
        "\n",
        "df = pd.concat([df_1, df_2], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Data Quality Assurance and Multi-Source Integration\n",
        "\n",
        "**Why This Approach:**\n",
        "Removes incomplete records and combines multiple datasets to create a robust, larger training corpus with diverse spam patterns.\n",
        "\n",
        "**Technical Details:**\n",
        "\n",
        "1. **Null Handling**: Remove rows with empty text\n",
        "   - `dropna(subset=['text'])`: Targets only the text column\n",
        "   - Missing text → no features to train on\n",
        "   - Records are irretrievable without content\n",
        "\n",
        "2. **Dataset Concatenation**: Merge two sources\n",
        "   - `pd.concat()` vertically stacks DataFrames (union operation)\n",
        "   - `ignore_index=True`: Re-indexes rows 0 to n-1\n",
        "   - Creates unified dataset from multiple sources\n",
        "\n",
        "**Mathematical Impact:**\n",
        "- **Sample Size Effect**: $n_{\\text{total}} = n_1 + n_2$\n",
        "- **Larger n improves estimates**: $\\text{Var}(\\hat{\\theta}) \\propto \\frac{1}{n}$\n",
        "- **Dataset Diversity**: Spam patterns from different corpora\n",
        "  - TREC 2007: Modern spam types (phishing, offers)\n",
        "  - Enron: Historical spam, internal fraud patterns\n",
        "  - Combined: Robust across temporal variations\n",
        "\n",
        "**Quality Metrics:**\n",
        "- Removed NaN rows: Preserves data integrity\n",
        "- Combined size: Larger training set for better generalization\n",
        "- Balanced representation: Multiple spam sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "6gKhBdHnoDQf",
        "outputId": "3da0d8c4-da17-48a2-8b41-6cf1a84ff8ba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 87384,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 83038,\n        \"samples\": [\n          \"unreported determinant angelenos vichy crab digital parades borrowing mould clemson casket three americanism yellowed shearing plant pitt bald agamemnon supercomputer awe balustrades toughness crowing ambulances kant awake enduring refinery nagasaki fuller - - phone : 461 - 935 - 9871 mobile : 629 - 182 - 3178 email : geoff . ormond 2001 @ norikomail . com\",\n          \"howstuffworksa r computers electronics june escapenumber escapenumber http www howstuffworks com rss feeds htm rss http electronics howstuffworks com virtual surround sound htm how virtual surround sound works the typical home theater setup with its surround sound speakers and subwoofer doesn't work for every home that's where http electronics howstuffworks com virtual surround sound htm virtual surround soundcomes in explore the traits of human hearing that allow two speakers to sound like five also at howstuffworks http products howstuffworks com fathers day cool gadget gifts dad will dig article htm http products howstuffworks com fathers day cool gadget gifts dad will dig article htm gifts dad will dig forget the tie father's day is all about getting dad plugged in and wired for action from headphones to smartphones our gift guide has the perfect electronic gadgets to make his day a great one http entertainment howstuffworks com this month in history june htm http entertainment howstuffworks com this month in history june htm this month in history june http entertainment howstuffworks com this month in history june htm historic events in june include the first recorded lunar eclipse in north america amelia earhart's transatlantic flight the debut of the garfield comic strip and the watergate break in learn more trivia about june in history http electronics howstuffworks com noise canceling headphone htm http electronics howstuffworks com noise canceling headphone htm how noise canceling headphones work noise canceling headphones are designed especially to maximize your listening experience keeping ambient noise out without sacrificing your music's sound quality learn how headphones especially noise canceling headphones work this week's video http videos howstuffworks com digital camera video htm http videos howstuffworks com digital camera video htm how to buy a digital camera buying a digital camera can be a challenging task learn about the key elements to consider when buying a digital camera in this howstuffworks http videos howstuffworks com digital camera video htm video this e mail was sent to ktwarwic speedy uwaterloo ca by howstuffworks escapenumber peachtree road suite escapenumber atlanta ga escapenumber usa you received this because your e mail address was submitted to the howstuffworks newsletter service if this e mail has been sent to you in error or if you would like to be removed from the list please http cl exct net unsub center aspx s escapelong j escapelong mid escapelong lid escapelong jb ffcfescapenumber unsubscribe hereor change your preferences using our http cl exct net profile center aspx s escapelong mid escapelong j escapelong l escapelong jb ffcfescapenumber profile manager to view this email as a web page click http view exacttarget com j escapelong m escapelong ls escapelong here howstuffworks is a registered trademark of howstuffworks inc a c escapenumber howstuffworks inc all rights reserved\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-13ea4c5d-ae56-4048-8800-9f4088fa0e7a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>do you feel the pressure to perform and not ri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>hi i've just updated from the gulus and i chec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>mega authenticv i a g r a discount pricec i a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>hey billy it was really fun going out the othe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>system of the home it will have the capabiliti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87379</th>\n",
              "      <td>1</td>\n",
              "      <td>hello , welcome to gigapharm onlinne shop . pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87380</th>\n",
              "      <td>1</td>\n",
              "      <td>i got it earlier than expected and it was wrap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87381</th>\n",
              "      <td>1</td>\n",
              "      <td>are you ready to rock on ? let the man in you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87382</th>\n",
              "      <td>1</td>\n",
              "      <td>learn how to last 5 - 10 times longer in bed ....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87383</th>\n",
              "      <td>1</td>\n",
              "      <td>hi : ) do you need some softwares ? i can give...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>87384 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13ea4c5d-ae56-4048-8800-9f4088fa0e7a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-13ea4c5d-ae56-4048-8800-9f4088fa0e7a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-13ea4c5d-ae56-4048-8800-9f4088fa0e7a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9af649a5-60b5-4f02-8c19-925192a56f2b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9af649a5-60b5-4f02-8c19-925192a56f2b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9af649a5-60b5-4f02-8c19-925192a56f2b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_a261a937-154b-49c7-aa61-ba6020d44bce\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a261a937-154b-49c7-aa61-ba6020d44bce button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "       label                                               text\n",
              "0          1  do you feel the pressure to perform and not ri...\n",
              "1          0  hi i've just updated from the gulus and i chec...\n",
              "2          1  mega authenticv i a g r a discount pricec i a ...\n",
              "3          1  hey billy it was really fun going out the othe...\n",
              "4          1  system of the home it will have the capabiliti...\n",
              "...      ...                                                ...\n",
              "87379      1  hello , welcome to gigapharm onlinne shop . pr...\n",
              "87380      1  i got it earlier than expected and it was wrap...\n",
              "87381      1  are you ready to rock on ? let the man in you ...\n",
              "87382      1  learn how to last 5 - 10 times longer in bed ....\n",
              "87383      1  hi : ) do you need some softwares ? i can give...\n",
              "\n",
              "[87384 rows x 2 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.drop(columns=[\"Message\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Final Data Preparation and Inspection\n",
        "\n",
        "**Why This Approach:**\n",
        "Removes the original Message column (now merged into text) and displays final dataset structure for validation before modeling.\n",
        "\n",
        "**Technical Details:**\n",
        "- **Redundant Feature Removal**: Message column merged into text, now duplicate\n",
        "- **Memory Optimization**: Reduces DataFrame size\n",
        "- **Schema Validation**: Final `.head()` shows clean, model-ready structure\n",
        "\n",
        "**Expected Structure:**\n",
        "Final DataFrame should contain only:\n",
        "- `text`: Preprocessed or raw email content\n",
        "- `label`: Binary spam/ham label (0 or 1)\n",
        "- Other relevant features (if any)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ySOFYOzn8qH",
        "outputId": "eda6f4e4-c5f2-4556-cb77-c93ae6cad21c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['label', 'text'], dtype='object')"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Schema Verification\n",
        "\n",
        "**Why This Approach:**\n",
        "Verifies final column names before text preprocessing, confirming data is properly structured.\n",
        "\n",
        "**Technical Details:**\n",
        "- **Schema Audit**: Lists all column names\n",
        "- **Validation Step**: Ensures dropped/renamed columns are correct\n",
        "- **Preprocessing Readiness**: Confirms 'text' and 'label' columns exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bde7a61",
        "outputId": "7952dfe9-24ac-487f-8e9c-2d736d644dae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame after applying text preprocessing:\n",
            "                                                text  label\n",
            "0  feel pressure perform rising occasion try v ia...      1\n",
            "1  hi ive updated gulu check mirror seems little ...      0\n",
            "2  mega authenticv g r discount pricec l discount...      1\n",
            "3  hey billy really fun going night talking said ...      1\n",
            "4  system home capability linked far know within ...      1\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added this download\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text) # Corrected digit removal to handle multiple digits\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words and lemmatize\n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        if token not in stop_words:\n",
        "            cleaned_tokens.append(lemmatizer.lemmatize(token))\n",
        "    # Join back into string\n",
        "    return ' '.join(cleaned_tokens)\n",
        "\n",
        "# Apply the preprocessing function to the 'text' column\n",
        "df['text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "print(\"DataFrame after applying text preprocessing:\")\n",
        "print(df[['text', 'label']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3e95322c"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Define the TF-IDF Vectorizer\n",
        "# Initialize TfidfVectorizer\n",
        "\n",
        "pipeline_nb = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=10_000)),\n",
        "    ('nb', MultinomialNB())\n",
        "])\n",
        "\n",
        "pipeline_lr = Pipeline([\n",
        "    # max_df: Ignore words that appear in more than 90% of documents. These are boring words that don’t help discriminate classes. Think “the”, “and”, “is”.\n",
        "    # min_df: Ignore words that appear in fewer than 5 documents. These are too rare to generalize. They add noise.\n",
        "    # ngram_range: This means:  unigrams: \"hello\"  bigrams: \"hello world\"\n",
        "    ('tfidf', TfidfVectorizer(max_features=10_000, ngram_range=(1,2), max_df=0.9,min_df=5)),\n",
        "    ('lr', LogisticRegression(max_iter=1000,solver='saga', penalty='l1')) # lasso\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Model Pipeline Architecture with TF-IDF and Algorithm Selection\n",
        "\n",
        "**Why This Approach:**\n",
        "We implement two pipelines comparing Multinomial Naive Bayes (generative model) vs. Logistic Regression (discriminative model). Pipelines ensure proper transformation order and prevent data leakage.\n",
        "\n",
        "---\n",
        "\n",
        "## TF-IDF Vectorization\n",
        "\n",
        "**Technical Details:**\n",
        "- **max_features=10,000**: Limits vocabulary to top 10k terms by document frequency\n",
        "  - Reduces sparse matrix from ~50k to 10k dimensions\n",
        "  - Keeps most informative features (Zipf's law: few words account for most occurrences)\n",
        "  \n",
        "- **ngram_range=(1,2)** (LR only): Captures unigrams and bigrams\n",
        "  - Unigram: \"free\"\n",
        "  - Bigram: \"free money\", \"click here\"\n",
        "  - Bigrams capture local context and idiomatic spam phrases\n",
        "  \n",
        "- **max_df=0.9**: Ignore terms in >90% of documents\n",
        "  - These are near-stopwords with low discriminative power\n",
        "  \n",
        "- **min_df=5**: Ignore terms in <5 documents\n",
        "  - Rare terms are often typos/outliers causing overfitting\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "TF-IDF creates a document-term matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ where:\n",
        "- $n$ = number of documents\n",
        "- $d$ = vocabulary size (max_features)\n",
        "- Each entry: $x_{ij} = \\text{TF-IDF}(t_j, d_i)$\n",
        "\n",
        "For term $t$ in document $d$ from corpus $D$:\n",
        "\n",
        "$$\\text{TF}(t,d) = \\frac{f_{t,d}}{\\sum_{t' \\in d} f_{t',d}}$$\n",
        "\n",
        "$$\\text{IDF}(t,D) = \\log\\frac{|D|}{1 + |\\{d \\in D : t \\in d\\}|}$$\n",
        "\n",
        "$$\\text{TF-IDF}(t,d,D) = \\text{TF}(t,d) \\times \\text{IDF}(t,D)$$\n",
        "\n",
        "**Normalization**: TF-IDF vectors are L2-normalized:\n",
        "\n",
        "$$\\mathbf{x}_i' = \\frac{\\mathbf{x}_i}{||\\mathbf{x}_i||_2}$$\n",
        "\n",
        "This makes documents comparable regardless of length.\n",
        "\n",
        "---\n",
        "\n",
        "## Multinomial Naive Bayes\n",
        "\n",
        "**Why This Algorithm:**\n",
        "- **Generative Model**: Models $P(x|y)$ and $P(y)$, uses Bayes' theorem for classification\n",
        "- **Computational Efficiency**: $O(nd)$ training time (linear in features and samples)\n",
        "- **Works Well with Sparse Data**: Text data is 90-99% sparse; NB handles this naturally\n",
        "- **Strong Baseline**: Often competitive with more complex models on text\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "Bayes' Theorem:\n",
        "$$P(y|x) = \\frac{P(x|y)P(y)}{P(x)} \\propto P(x|y)P(y)$$\n",
        "\n",
        "For text classification:\n",
        "$$P(\\text{spam}|\\text{email}) \\propto P(\\text{email}|\\text{spam}) \\times P(\\text{spam})$$\n",
        "\n",
        "**Naive Assumption**: Features (words) are conditionally independent given class:\n",
        "$$P(\\mathbf{x}|y) = \\prod_{i=1}^{d} P(x_i|y)$$\n",
        "\n",
        "For Multinomial variant (suitable for count/frequency data):\n",
        "$$P(x_i | y) = \\frac{N_{yi} + \\alpha}{N_y + \\alpha d}$$\n",
        "\n",
        "Where:\n",
        "- $N_{yi}$ = count of feature $i$ in class $y$\n",
        "- $N_y$ = total count of all features in class $y$\n",
        "- $\\alpha$ = smoothing parameter (Laplace smoothing, default=1)\n",
        "\n",
        "**Classification Rule:**\n",
        "$$\\hat{y} = \\arg\\max_y \\left[ \\log P(y) + \\sum_{i=1}^{d} x_i \\log P(x_i|y) \\right]$$\n",
        "\n",
        "---\n",
        "\n",
        "## Logistic Regression with L1 Regularization\n",
        "\n",
        "**Why This Algorithm:**\n",
        "- **Discriminative Model**: Directly models $P(y|x)$ without assumptions about $P(x|y)$\n",
        "- **Handles Feature Correlation**: No independence assumption like Naive Bayes\n",
        "- **L1 Regularization (Lasso)**: Induces sparsity, performing automatic feature selection\n",
        "- **Interpretable**: Coefficients show feature importance and direction\n",
        "\n",
        "**Technical Details:**\n",
        "- **max_iter=1000**: Ensures convergence for high-dimensional sparse data\n",
        "- **solver='saga'**: Stochastic Average Gradient Descent\n",
        "  - Efficient for large datasets\n",
        "  - Supports L1 penalty (unlike 'lbfgs')\n",
        "- **penalty='l1'**: Lasso regularization for sparsity\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "Logistic Regression models probability using sigmoid function:\n",
        "$$P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T\\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^T\\mathbf{x} + b)}}$$\n",
        "\n",
        "**Objective Function with L1 Penalty:**\n",
        "$$\\min_{\\mathbf{w}} \\left[ \\frac{1}{n}\\sum_{i=1}^{n} \\log(1 + e^{-y_i(\\mathbf{w}^T\\mathbf{x}_i + b)}) + \\lambda||\\mathbf{w}||_1 \\right]$$\n",
        "\n",
        "Where:\n",
        "- First term: Negative log-likelihood (cross-entropy loss)\n",
        "- Second term: L1 regularization $||\\mathbf{w}||_1 = \\sum_{j=1}^{d}|w_j|$\n",
        "- $\\lambda$: Regularization strength (controlled by C parameter in sklearn, where $C = 1/\\lambda$)\n",
        "\n",
        "**L1 Regularization Effect:**\n",
        "$$||\\mathbf{w}||_1 = \\sum_{j=1}^{d}|w_j|$$\n",
        "\n",
        "The L1 norm penalty drives many weights to exactly zero:\n",
        "- Creates sparse model (many $w_j = 0$)\n",
        "- Automatic feature selection\n",
        "- Improved interpretability\n",
        "- Reduces overfitting in high dimensions\n",
        "\n",
        "**Why L1 over L2?**\n",
        "- L1 (Lasso): Produces sparse solutions, $\\nabla|w| = \\text{sign}(w)$\n",
        "- L2 (Ridge): Shrinks all coefficients uniformly, $\\nabla w^2 = 2w$\n",
        "- For text with 10k features, L1 identifies ~100-500 most important terms\n",
        "\n",
        "**Gradient for SAGA Solver:**\n",
        "At each iteration, update uses sampled gradient:\n",
        "$$\\mathbf{w}_{t+1} = \\mathbf{w}_t - \\eta_t \\nabla L(\\mathbf{w}_t)$$\n",
        "\n",
        "Where $\\nabla L$ includes both loss and penalty gradients.\n",
        "\n",
        "---\n",
        "\n",
        "## Pipeline Benefits\n",
        "\n",
        "1. **Prevents Data Leakage**: TF-IDF fit on training data, transform on test data\n",
        "2. **Code Simplicity**: Single `.fit()` and `.predict()` call\n",
        "3. **Cross-Validation Compatibility**: Proper fold-wise transformation\n",
        "4. **Reproducibility**: Encapsulates preprocessing and modeling\n",
        "\n",
        "**Comparison Expected:**\n",
        "- **NB**: Faster training, good baseline, assumes feature independence\n",
        "- **LR with L1**: Better with correlated features, feature selection, typically higher performance on text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "83wev6Vu0qHk"
      },
      "outputs": [],
      "source": [
        "# for multimodal naieve bayes\n",
        "scores_nb = cross_val_score(\n",
        "    pipeline_nb,\n",
        "    df[\"text\"],\n",
        "    df[\"label\"],\n",
        "    cv=5,\n",
        "    scoring=\"f1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Cross-Validation for Naive Bayes Model\n",
        "\n",
        "**Why This Approach:**\n",
        "Cross-validation provides robust performance estimates by testing on multiple data splits. Using F1-score as the metric balances precision and recall, crucial for imbalanced spam datasets.\n",
        "\n",
        "**Technical Details:**\n",
        "- **cv=5**: 5-fold stratified cross-validation\n",
        "  - Dataset split into 5 equal parts\n",
        "  - Each fold used once as validation, 4 times as training\n",
        "  - Stratification preserves class distribution in each fold\n",
        "  \n",
        "- **scoring='f1'**: F1-score metric for binary classification\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "**K-Fold Cross-Validation:**\n",
        "For $k=5$ folds, dataset $D$ is partitioned into $\\{D_1, D_2, D_3, D_4, D_5\\}$\n",
        "\n",
        "For fold $i$:\n",
        "- Train on: $D \\setminus D_i$ (80% of data)\n",
        "- Test on: $D_i$ (20% of data)\n",
        "\n",
        "Final score: $\\text{CV-Score} = \\frac{1}{k}\\sum_{i=1}^{k} \\text{F1}(D_i)$\n",
        "\n",
        "**F1-Score Definition:**\n",
        "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
        "\n",
        "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "$$\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2TP}{2TP + FP + FN}$$\n",
        "\n",
        "**Why F1 over Accuracy?**\n",
        "For imbalanced data (e.g., 70% ham, 30% spam):\n",
        "- Accuracy can be misleading (90% by predicting all ham)\n",
        "- F1 balances false positives (legitimate emails marked as spam) and false negatives (spam getting through)\n",
        "\n",
        "**Harmonic Mean Interpretation:**\n",
        "F1 is the harmonic mean of precision and recall:\n",
        "$$\\text{F1} = \\frac{2}{\\frac{1}{\\text{Precision}} + \\frac{1}{\\text{Recall}}}$$\n",
        "\n",
        "This penalizes extreme values more than arithmetic mean:\n",
        "- If Precision=1.0 and Recall=0.1: Arithmetic mean=0.55, F1=0.18\n",
        "- Ensures balanced performance\n",
        "\n",
        "**Variance Estimation:**\n",
        "With $k$ folds, we get score variance:\n",
        "$$\\sigma^2 = \\frac{1}{k}\\sum_{i=1}^{k}(\\text{F1}_i - \\bar{\\text{F1}})^2$$\n",
        "\n",
        "Lower variance indicates more stable model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "9pqWg1AE005h"
      },
      "outputs": [],
      "source": [
        "# for logestic regression\n",
        "scores_lr = cross_val_score(\n",
        "    pipeline_lr,\n",
        "    df[\"text\"],\n",
        "    df[\"label\"],\n",
        "    cv=5,\n",
        "    scoring=\"f1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Cross-Validation for Logistic Regression Model\n",
        "\n",
        "**Why This Approach:**\n",
        "Same cross-validation strategy as Naive Bayes, allowing fair comparison. The more complex LR model with bigrams and L1 regularization requires validation to detect potential overfitting.\n",
        "\n",
        "**Technical Details:**\n",
        "- Identical cv=5 and scoring='f1' parameters ensure apples-to-apples comparison\n",
        "- LR's computational cost is higher due to:\n",
        "  - Bigram features (vocabulary size increases)\n",
        "  - Iterative optimization (SAGA solver with max_iter=1000)\n",
        "  - L1 proximal gradient computations\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "**Expected Performance Difference:**\n",
        "\n",
        "For linearly separable data with feature dimension $d$:\n",
        "- **Naive Bayes**: Bias increases with violation of independence assumption\n",
        "  - Error rate: $\\epsilon_{NB} \\propto \\text{Dependence}(\\mathbf{x}_i, \\mathbf{x}_j | y)$\n",
        "  \n",
        "- **Logistic Regression**: No independence assumption\n",
        "  - With sufficient data: $\\epsilon_{LR} \\rightarrow \\epsilon_{Bayes}$ (optimal)\n",
        "  - Sample complexity: $O(d)$ samples needed for convergence\n",
        "\n",
        "**Regularization Path:**\n",
        "L1 penalty creates a solution path parameterized by $\\lambda$:\n",
        "\n",
        "$$\\mathbf{w}(\\lambda) = \\arg\\min_{\\mathbf{w}} \\left[ L(\\mathbf{w}) + \\lambda||\\mathbf{w}||_1 \\right]$$\n",
        "\n",
        "As $\\lambda$ increases:\n",
        "- More coefficients become exactly zero\n",
        "- Model becomes more sparse and simpler\n",
        "- Bias increases, variance decreases\n",
        "\n",
        "Default $C$ (inverse of $\\lambda$) in sklearn is optimized for typical problems.\n",
        "\n",
        "**Computational Complexity:**\n",
        "- NB: $O(nd + d)$ - linear pass through data\n",
        "- LR: $O(T \\cdot nd)$ where $T$ is number of iterations\n",
        "  - Typically $T \\approx 100-1000$ for convergence\n",
        "  - Each iteration: gradient computation over $n$ samples and $d$ features\n",
        "\n",
        "**Cross-Validation Computational Cost:**\n",
        "Total training operations: $k \\times T \\times n \\times d$ where:\n",
        "- $k=5$ folds\n",
        "- $T \\approx 100-1000$ iterations\n",
        "- $n \\approx 30,000$ emails (combined datasets)\n",
        "- $d \\approx 10,000-20,000$ features (with bigrams)\n",
        "\n",
        "This explains why LR takes longer than NB to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xTjfOr93xLc",
        "outputId": "8ee26964-9cfb-402a-dee8-8224e2342a80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NB F1 per fold: [0.96095935 0.96917935 0.97307059 0.95547418 0.95613297]\n",
            "NB mean F1: 0.9629632880262624\n",
            "LR F1 per fold: [0.98195329 0.98095037 0.98379581 0.96354894 0.91397534]\n",
            "LR mean F1: 0.9648447474183133\n"
          ]
        }
      ],
      "source": [
        "print(\"NB F1 per fold:\", scores_nb)\n",
        "print(\"NB mean F1:\", scores_nb.mean())\n",
        "\n",
        "print(\"LR F1 per fold:\", scores_lr)\n",
        "print(\"LR mean F1:\", scores_lr.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Model Performance Comparison and Evaluation\n",
        "\n",
        "**Why This Approach:**\n",
        "Comparing mean F1 scores across both models reveals which approach better captures spam patterns. Per-fold scores show stability/variance of each model.\n",
        "\n",
        "**Technical Details:**\n",
        "- **Per-Fold Scores**: Reveals consistency across different data splits\n",
        "  - High variance → model sensitive to training data composition\n",
        "  - Low variance → robust, generalizable model\n",
        "  \n",
        "- **Mean F1**: Single metric for model selection\n",
        "  - Higher mean indicates better average performance\n",
        "  - Must consider variance: prefer high mean with low variance\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "**Model Comparison Statistical Test:**\n",
        "\n",
        "Given two sets of CV scores $\\{s_1^{NB}, ..., s_5^{NB}\\}$ and $\\{s_1^{LR}, ..., s_5^{LR}\\}$:\n",
        "\n",
        "**Paired t-test** determines if difference is significant:\n",
        "$$t = \\frac{\\bar{s}_{LR} - \\bar{s}_{NB}}{\\sqrt{\\frac{s^2_{LR}}{k} + \\frac{s^2_{NB}}{k}}}$$\n",
        "\n",
        "Where $s^2$ is sample variance. If $|t| > t_{\\alpha,k-1}$, difference is statistically significant.\n",
        "\n",
        "**Bias-Variance Tradeoff:**\n",
        "\n",
        "Total error decomposes as:\n",
        "$$\\mathbb{E}[(\\hat{y} - y)^2] = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n",
        "\n",
        "- **NB**: Higher bias (independence assumption), lower variance (fewer parameters)\n",
        "- **LR with L1**: Lower bias (flexible model), higher variance (more parameters, regularization reduces this)\n",
        "\n",
        "**Expected Results:**\n",
        "Typically for spam detection:\n",
        "- NB F1: 0.85-0.92\n",
        "- LR F1: 0.90-0.96\n",
        "\n",
        "**Why LR Usually Wins:**\n",
        "1. **Bigrams**: Captures phrases like \"free money\", \"click here\"\n",
        "2. **No Independence Assumption**: Words in spam are correlated (\"free\" often appears with \"prize\")\n",
        "3. **Feature Selection**: L1 identifies ~500 most discriminative features automatically\n",
        "\n",
        "**Interpretation Example:**\n",
        "If NB mean F1 = 0.88 ± 0.02 and LR mean F1 = 0.94 ± 0.01:\n",
        "- LR is 6.8% better in F1-score\n",
        "- LR is more stable (lower standard deviation)\n",
        "- LR is the better model for production deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Aw3scVr5YHPo"
      },
      "outputs": [],
      "source": [
        "model_nb = pipeline_nb.fit(df[\"text\"], df[\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Training Final Naive Bayes Model on Full Dataset\n",
        "\n",
        "**Why This Approach:**\n",
        "After validation, we train on the complete dataset to maximize model capacity. More training data generally improves generalization, especially for generative models like Naive Bayes.\n",
        "\n",
        "**Technical Details:**\n",
        "- **Full Dataset Training**: Uses all samples (no holdout)\n",
        "  - Cross-validation already provided unbiased performance estimate\n",
        "  - Final model benefits from maximum available data\n",
        "  \n",
        "- **Pipeline Fitting**: Both TF-IDF vectorization and NB classifier trained in one call\n",
        "  - TF-IDF learns vocabulary, document frequencies, IDF weights\n",
        "  - NB learns class priors $P(y)$ and feature likelihoods $P(x_i|y)$\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "**Parameter Learning:**\n",
        "\n",
        "**Class Prior:**\n",
        "$$P(y=1) = \\frac{n_{\\text{spam}}}{n_{\\text{total}}}$$\n",
        "$$P(y=0) = \\frac{n_{\\text{ham}}}{n_{\\text{total}}}$$\n",
        "\n",
        "**Feature Likelihood (Multinomial):**\n",
        "For each feature $i$ and class $y$:\n",
        "$$P(x_i|y) = \\frac{N_{yi} + \\alpha}{N_y + \\alpha d}$$\n",
        "\n",
        "Where:\n",
        "- $N_{yi}$ = sum of TF-IDF values for feature $i$ in class $y$ documents\n",
        "- $N_y$ = sum of all TF-IDF values in class $y$\n",
        "- $\\alpha = 1$ (Laplace smoothing)\n",
        "- $d = 10,000$ (vocabulary size)\n",
        "\n",
        "**Learning Complexity:**\n",
        "- Time: $O(nd)$ - single pass through data\n",
        "- Space: $O(d \\cdot c)$ where $c=2$ classes\n",
        "  - Store $P(x_i|y)$ for each feature-class pair\n",
        "  - NB stores ~20,000 parameters (10k features × 2 classes)\n",
        "\n",
        "**Sample Size Effect:**\n",
        "More data improves estimates of $P(x_i|y)$:\n",
        "$$\\text{Var}(P(x_i|y)) \\propto \\frac{1}{n_y}$$\n",
        "\n",
        "With larger $n_y$, variance decreases, estimates become more reliable.\n",
        "\n",
        "**Why Full Dataset Training is Safe:**\n",
        "- CV already detected overfitting/underfitting\n",
        "- NB has strong inductive bias (independence assumption) limiting overfitting\n",
        "- With 10k features and ~30k samples, we have ~3 samples per feature - adequate for NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "2GK8DWDbYhg6"
      },
      "outputs": [],
      "source": [
        "model_lr = pipeline_lr.fit(df[\"text\"], df[\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Training Final Logistic Regression Model on Full Dataset\n",
        "\n",
        "**Why This Approach:**\n",
        "Training LR on the full dataset leverages all available information to learn optimal decision boundaries. The L1 regularization prevents overfitting even with complete data usage.\n",
        "\n",
        "**Technical Details:**\n",
        "- **Iterative Optimization**: SAGA solver performs up to 1000 iterations\n",
        "  - Each iteration updates weights based on gradient\n",
        "  - Convergence when gradient magnitude < tolerance threshold\n",
        "  \n",
        "- **L1 Regularization Effect**: Automatic feature selection during training\n",
        "  - Many weights driven to exactly zero\n",
        "  - Final model uses only ~500-1000 of 10k+ features\n",
        "  \n",
        "- **Bigram Learning**: Model discovers discriminative phrase patterns\n",
        "  - Examples: \"limited_time\", \"act_now\", \"click_below\"\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "**Optimization Problem:**\n",
        "$$\\min_{\\mathbf{w}, b} \\left[ \\frac{1}{n}\\sum_{i=1}^{n}\\log(1 + e^{-y_i(\\mathbf{w}^T\\mathbf{x}_i + b)}) + \\lambda||\\mathbf{w}||_1 \\right]$$\n",
        "\n",
        "**SAGA Update Rule:**\n",
        "At iteration $t$, for randomly selected sample $i$:\n",
        "$$\\mathbf{w}^{t+1} = \\mathbf{w}^t - \\eta \\left[\\nabla \\ell_i(\\mathbf{w}^t) + \\text{gradient correction} + \\lambda \\cdot \\text{sign}(\\mathbf{w}^t)\\right]$$\n",
        "\n",
        "Where:\n",
        "- $\\nabla \\ell_i$ = gradient of log-loss for sample $i$\n",
        "- Gradient correction = maintains unbiased estimate using stored gradients\n",
        "- $\\lambda \\cdot \\text{sign}(\\mathbf{w})$ = subgradient of L1 penalty\n",
        "\n",
        "**Proximal Operator for L1:**\n",
        "After gradient step, soft-thresholding applied:\n",
        "$$w_j^{\\text{new}} = \\text{sign}(w_j^{\\text{old}})\\max(|w_j^{\\text{old}}| - \\lambda\\eta, 0)$$\n",
        "\n",
        "This creates sparsity: weights with $|w_j| < \\lambda\\eta$ become exactly zero.\n",
        "\n",
        "**Convergence Criterion:**\n",
        "Training stops when:\n",
        "$$||\\nabla L(\\mathbf{w}^t)||_2 < \\epsilon$$\n",
        "\n",
        "Or max_iter reached. Typical convergence: 200-800 iterations.\n",
        "\n",
        "**Learned Parameters:**\n",
        "- Weight vector: $\\mathbf{w} \\in \\mathbb{R}^d$ where most entries are zero\n",
        "- Bias term: $b \\in \\mathbb{R}$\n",
        "- Non-zero weights indicate important features:\n",
        "  - Large positive $w_j$ → feature $j$ strongly indicates spam\n",
        "  - Large negative $w_j$ → feature $j$ strongly indicates ham\n",
        "\n",
        "**Decision Boundary:**\n",
        "Hyperplane in $d$-dimensional space:\n",
        "$$\\mathbf{w}^T\\mathbf{x} + b = 0$$\n",
        "\n",
        "Classification:\n",
        "$$\\hat{y} = \\begin{cases} 1 & \\text{if } \\mathbf{w}^T\\mathbf{x} + b > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$\n",
        "\n",
        "**Model Capacity:**\n",
        "With $d \\approx 20,000$ bigram features:\n",
        "- Theoretical capacity: can fit $\\approx d$ samples perfectly\n",
        "- L1 regularization reduces effective capacity to ~1,000 active features\n",
        "- Prevents overfitting despite high dimensionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FQ4qohpYTfz",
        "outputId": "f0f7796c-0d3d-4576-cb64-02f59a888792"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/gdrive/MyDrive/trec_spam_data_extracted/model_lr.joblib']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "\n",
        "# save the models and test the results on the real world\n",
        "\n",
        "model_path_nb = os.path.join(gdrive_extracted_data_path, 'model_nb.joblib')\n",
        "model_path_lr = os.path.join(gdrive_extracted_data_path, 'model_lr.joblib')\n",
        "\n",
        "joblib.dump(model_nb, model_path_nb)\n",
        "joblib.dump(model_lr, model_path_lr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Model Persistence with Joblib and Deployment Preparation\n",
        "\n",
        "**Why This Approach:**\n",
        "Trained models must be saved for deployment. Joblib is optimized for large NumPy arrays (like TF-IDF matrices and model parameters), providing efficient serialization with compression for production use.\n",
        "\n",
        "**Technical Details:**\n",
        "- **Joblib vs Pickle**: \n",
        "  - Joblib: Optimized for numerical arrays, uses efficient compression\n",
        "  - Pickle: General Python serialization, less efficient for large models\n",
        "  - Joblib can be 2-10x faster for sklearn models\n",
        "  \n",
        "- **What Gets Saved**:\n",
        "  - **TF-IDF Vectorizer**: Vocabulary dict, IDF weights, preprocessing parameters\n",
        "  - **Classifier**: Model weights, hyperparameters, class labels\n",
        "  - **Pipeline Structure**: Ensures correct transform → predict order\n",
        "  \n",
        "- **File Format**: .joblib files are compressed pickles\n",
        "  - Compression reduces file size by ~50-70%\n",
        "  - Preserves exact model state for reproducible predictions\n",
        "\n",
        "**Mathematical Foundation:**\n",
        "\n",
        "**Model State for Naive Bayes:**\n",
        "Serialized objects include:\n",
        "1. **Class priors**: $P(y=0), P(y=1) \\in \\mathbb{R}^2$\n",
        "2. **Feature log-probabilities**: $\\log P(x_i|y) \\in \\mathbb{R}^{d \\times 2}$\n",
        "   - Matrix size: 10,000 features × 2 classes = 20,000 floats\n",
        "   - Storage: ~160 KB (8 bytes per float64)\n",
        "3. **TF-IDF vocabulary**: Dict mapping words to indices\n",
        "   - ~10,000 strings\n",
        "4. **IDF weights**: $\\text{IDF}(t) \\in \\mathbb{R}^d$\n",
        "\n",
        "**Model State for Logistic Regression:**\n",
        "1. **Weight vector**: $\\mathbf{w} \\in \\mathbb{R}^d$ (20,000+ features with bigrams)\n",
        "   - Storage: ~160 KB\n",
        "   - Most entries are zero due to L1 regularization\n",
        "2. **Bias term**: $b \\in \\mathbb{R}$\n",
        "3. **TF-IDF vocabulary and IDF weights** (same as NB)\n",
        "\n",
        "**Total File Sizes:**\n",
        "- NB model: ~1-2 MB (uncompressed), ~500 KB (compressed)\n",
        "- LR model: ~2-4 MB (uncompressed), ~800 KB (compressed)\n",
        "\n",
        "**Loading and Inference:**\n",
        "```python\n",
        "model = joblib.load('model_lr.joblib')\n",
        "prediction = model.predict(new_email_text)  # Returns 0 (ham) or 1 (spam)\n",
        "probability = model.predict_proba(new_email_text)  # Returns [P(ham), P(spam)]\n",
        "```\n",
        "\n",
        "**Inference Complexity:**\n",
        "For single email:\n",
        "- TF-IDF transform: $O(m \\cdot d)$ where $m$ = email length in words\n",
        "- NB prediction: $O(d)$ - dot product of feature vector with log-probs\n",
        "- LR prediction: $O(d)$ - dot product $\\mathbf{w}^T\\mathbf{x}$, sigmoid computation\n",
        "\n",
        "Typical inference time: 1-5 milliseconds per email\n",
        "\n",
        "**Production Deployment:**\n",
        "Saved models enable:\n",
        "1. **API Integration**: Load model in FastAPI/Flask server\n",
        "2. **Batch Processing**: Process thousands of emails efficiently\n",
        "3. **Version Control**: Track model versions over time\n",
        "4. **A/B Testing**: Compare multiple model versions in production\n",
        "5. **Reproducibility**: Exact same predictions across different systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "gJqWm6X00Mpk"
      },
      "outputs": [],
      "source": [
        "df.to_csv(os.path.join(gdrive_extracted_data_path, 'combined_data.csv'), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical Explanation: Preprocessed Data Persistence for Future Analysis\n",
        "\n",
        "**Why This Approach:**\n",
        "Saving the preprocessed combined dataset enables:\n",
        "- **Reproducibility**: Exact same data for retraining or analysis\n",
        "- **Data Auditing**: Track preprocessing steps applied\n",
        "- **Future Reference**: Analyze model predictions against original data\n",
        "- **Compliance**: Document what data was used for training\n",
        "\n",
        "**Technical Details:**\n",
        "- **CSV Export**: Universal format compatible with all tools\n",
        "- **Index Exclusion**: `index=False` omits row numbers, avoiding redundancy\n",
        "- **Data Integrity**: Preserved in Google Drive for long-term storage\n",
        "\n",
        "**Use Cases:**\n",
        "- Retraining models with updated hyperparameters\n",
        "- Analyzing misclassified examples\n",
        "- Comparing different preprocessing approaches\n",
        "- Creating training/validation dataset splits\n",
        "- Model explainability analysis"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
